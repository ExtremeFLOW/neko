module comm
  use mpi_f08, only : MPI_Comm, MPI_Datatype, MPI_THREAD_MULTIPLE, &
       MPI_THREAD_SERIALIZED, MPI_Comm_rank, MPI_Comm_split, MPI_Comm_dup,&
       MPI_Comm_rank, MPI_Comm_size, MPI_Comm_free, MPI_Barrier, &
       MPI_Finalize, MPI_Init, MPI_REAL, MPI_DOUBLE_PRECISION, MPI_COMM_WORLD
  use utils, only : neko_error
  use neko_config
  !$ use omp_lib
  implicit none
  private
  
  interface
     subroutine neko_comm_wrapper_init(fcomm) &
          bind(c, name='neko_comm_wrapper_init')
       integer, value :: fcomm
     end subroutine neko_comm_wrapper_init
  end interface

#ifdef HAVE_NCCL
  interface
     subroutine neko_comm_nccl_init() &
          bind(c, name='neko_comm_nccl_init')
     end subroutine neko_comm_nccl_init
  end interface

    interface
     subroutine neko_comm_nccl_finalize() &
          bind(c, name='neko_comm_nccl_finalize')
     end subroutine neko_comm_nccl_finalize
  end interface
#endif
  
  !> MPI communicator
  type(MPI_Comm), public :: NEKO_COMM

  !> MPI type for working precision of REAL types
#ifdef HAVE_MPI_PARAM_DTYPE
  type(MPI_Datatype), parameter :: MPI_REAL_PRECISION = @NEKO_MPI_REAL_TYPE@
  type(MPI_Datatype), parameter :: MPI_EXTRA_PRECISION = @NEKO_MPI_XREAL_TYPE@
#else
  type(MPI_Datatype) :: MPI_REAL_PRECISION
  type(MPI_Datatype) :: MPI_EXTRA_PRECISION
#endif

  public :: MPI_REAL_PRECISION, MPI_EXTRA_PRECISION
  
  !> MPI rank
  integer, public :: pe_rank

  !> MPI size of communicator
  integer, public :: pe_size

  public :: comm_init, comm_free
  
contains
  
  subroutine comm_init
    integer :: ierr
    logical :: initialized
    integer :: provided, nthrds

    pe_rank = -1
    pe_size = 0

    call MPI_Initialized(initialized, ierr)

    nthrds = 1
    !$omp parallel
    !$omp master
    !$ nthrds = omp_get_num_threads()
    !$omp end master
    !$omp end parallel
    
    if (.not.initialized) then       
       if (nthrds .gt. 1) then
          call MPI_Init_thread(MPI_THREAD_MULTIPLE, provided, ierr)
          if (provided .ne. MPI_THREAD_MULTIPLE) then
             ! MPI_THREAD_MULTIPLE is required for mt. device backends
             if (NEKO_BCKND_DEVICE .eq. 1) then 
                call neko_error('Invalid thread support provided by MPI')
             else
                call MPI_Init_thread(MPI_THREAD_SERIALIZED, provided, ierr)
                if (provided .ne. MPI_THREAD_SERIALIZED) then
                   call neko_error('Invalid thread support provided by MPI')
                end if
             end if
          end if
       else
          call MPI_Init(ierr)
       end if
    end if

#ifndef HAVE_MPI_PARAM_DTYPE
    MPI_REAL_PRECISION = @NEKO_MPI_REAL_TYPE@
    MPI_EXTRA_PRECISION = @NEKO_MPI_XREAL_TYPE@
#endif
    

#ifdef HAVE_ADIOS2
    ! We split the communicator it to work asynchronously (MPMD)
    call MPI_Comm_rank(MPI_COMM_WORLD, pe_rank, ierr)
    call MPI_Comm_split(MPI_COMM_WORLD, 0, pe_rank, NEKO_COMM, ierr)
#else    
    ! Original version duplicates the communicator:
    call MPI_Comm_dup(MPI_COMM_WORLD, NEKO_COMM, ierr)
#endif

    call MPI_Comm_rank(NEKO_COMM, pe_rank, ierr)
    call MPI_Comm_size(NEKO_COMM, pe_size, ierr)

    ! Setup C/C++ wrapper
    call neko_comm_wrapper_init(NEKO_COMM%mpi_val)


#ifdef HAVE_NCCL
    ! Setup NCCL (if requested)
    call neko_comm_nccl_init()
#endif

  end subroutine comm_init

  subroutine comm_free
    integer :: ierr

    call MPI_Barrier(NEKO_COMM, ierr)
    call MPI_Comm_free(NEKO_COMM, ierr)

#ifdef HAVE_NCCL
    call neko_comm_nccl_finalize()
#endif
    
    call MPI_Finalize(ierr)

  end subroutine comm_free

end module comm
